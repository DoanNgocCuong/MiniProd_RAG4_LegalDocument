{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export @src  th√†nh 1 c√°i API c·ªßa chatbot ch∆∞a c√≥ UI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert your existing chatbot code into an API without a UI, you can use a framework like Flask. Below are the necessary changes to create a simple API for your chatbot. This will allow you to send requests to the chatbot and receive responses without a user interface.\n",
    "\n",
    "### Changes Overview\n",
    "1. **Create a Flask app** to handle incoming requests.\n",
    "2. **Define an endpoint** to receive user questions and return chatbot responses.\n",
    "3. **Remove Streamlit-specific code** since we are not using a UI.\n",
    "\n",
    "### Updated Code\n",
    "Here‚Äôs how you can modify your `src/server.py` file:\n",
    "\n",
    "```python:src/server.py\n",
    "from flask import Flask, request, jsonify  # Import Flask\n",
    "from rag_pipeline.back import LLMHandler, VectorDatabase, QuestionAnsweringChain\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Parameters\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv('gemini_key')\n",
    "qdrant_key = os.getenv('qdrant_key')\n",
    "rerank = True\n",
    "rewrite = True\n",
    "num_docs = 5\n",
    "\n",
    "# Initialize components\n",
    "vector_db = VectorDatabase(\n",
    "    model_name=\"hiieu/halong_embedding\",\n",
    "    collection_name='cmc_final_db',\n",
    "    api=qdrant_key\n",
    ")\n",
    "llm_handler = LLMHandler(model_name=\"gemini-1.5-flash\", gemini_key=gemini_key)\n",
    "qa_chain = QuestionAnsweringChain(\n",
    "    llm_handler=llm_handler,\n",
    "    vector_db=vector_db,\n",
    "    num_docs=num_docs,\n",
    "    apply_rerank=rerank,\n",
    "    apply_rewrite=rewrite,\n",
    "    date_impact=0.001\n",
    ")\n",
    "\n",
    "@app.route('/chat', methods=['POST'])  # Define the chat endpoint\n",
    "def chat():\n",
    "    data = request.json\n",
    "    question = data.get('question')\n",
    "    \n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "\n",
    "    # Get AI response\n",
    "    response, extracted_links = qa_chain.run(question)\n",
    "\n",
    "    return jsonify({\n",
    "        \"response\": response,\n",
    "        \"links\": extracted_links\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)  # Run the Flask app\n",
    "```\n",
    "\n",
    "### Explanation of Changes\n",
    "- **Flask Setup**: The code imports Flask and initializes a Flask app.\n",
    "- **Chat Endpoint**: A new endpoint `/chat` is created to handle POST requests. It expects a JSON body with a `question` field.\n",
    "- **Response Handling**: The chatbot's response and any extracted links are returned as a JSON response.\n",
    "- **Removed Streamlit Code**: All Streamlit-specific code has been removed since we are now using an API.\n",
    "\n",
    "### Running the API\n",
    "To run the API, make sure you have Flask installed. You can install it using:\n",
    "```bash\n",
    "pip install Flask\n",
    "```\n",
    "Then, run your server:\n",
    "```bash\n",
    "python src/server.py\n",
    "```\n",
    "\n",
    "You can now send POST requests to `http://127.0.0.1:5000/chat` with a JSON body like:\n",
    "```json\n",
    "{\n",
    "    \"question\": \"Your question here\"\n",
    "}\n",
    "```\n",
    "\n",
    "This will return a JSON response with the chatbot's answer and any related links.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: update server.py (RAG) : t·ª´ vi·ªác m·ªói l·∫ßn ping port 3000 l·∫°i load l·∫°i to√†n b·ªô t·ª´ model ƒë·∫øn database, ... ===>>> chuy·ªÉn th√†nh: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "V·∫•n ƒë·ªÅ n√†y x·∫£y ra v√¨ Flask ƒëang ch·∫°y trong debug mode (`debug=True`), khi·∫øn n√≥ t·ª± ƒë·ªông reload khi c√≥ thay ƒë·ªïi. ƒê·ªÉ kh·∫Øc ph·ª•c, c√≥ 2 c√°ch:\n",
    "\n",
    "1. **C√°ch 1: T·∫Øt debug mode**\n",
    "\n",
    "```python:src/server.py\n",
    "if __name__ == '__main__':\n",
    "    logger.info(\"üöÄ Starting Flask application...\")\n",
    "    app.run(host='0.0.0.0', port=3000, debug=False)  # Set debug=False\n",
    "```\n",
    "\n",
    "2. **C√°ch 2: T·ªëi ∆∞u vi·ªác kh·ªüi t·∫°o model (Recommended)**\n",
    "\n",
    "```python:src/server.py\n",
    "# Initialize components outside of routes\n",
    "load_dotenv()\n",
    "gemini_key = os.getenv('gemini_key')\n",
    "qdrant_key = os.getenv('qdrant_key')\n",
    "\n",
    "# Global variables\n",
    "vector_db = None\n",
    "llm_handler = None\n",
    "qa_chain = None\n",
    "\n",
    "def initialize_components():\n",
    "    global vector_db, llm_handler, qa_chain\n",
    "    if vector_db is None:\n",
    "        vector_db = VectorDatabase(\n",
    "            model_name=\"hiieu/halong_embedding\",\n",
    "            collection_name='cmc_final_db',\n",
    "            api=qdrant_key\n",
    "        )\n",
    "    if llm_handler is None:\n",
    "        llm_handler = LLMHandler(model_name=\"gemini-1.5-flash\", gemini_key=gemini_key)\n",
    "    if qa_chain is None:\n",
    "        qa_chain = QuestionAnsweringChain(\n",
    "            llm_handler=llm_handler,\n",
    "            vector_db=vector_db,\n",
    "            num_docs=5,\n",
    "            apply_rerank=True,\n",
    "            apply_rewrite=True,\n",
    "            date_impact=0.001\n",
    "        )\n",
    "\n",
    "# Initialize once when starting\n",
    "initialize_components()\n",
    "\n",
    "@app.route('/chat', methods=['POST'])\n",
    "def chat():\n",
    "    data = request.json\n",
    "    question = data.get('question')\n",
    "    \n",
    "    if not question:\n",
    "        return jsonify({\"error\": \"No question provided\"}), 400\n",
    "\n",
    "    # Use global qa_chain\n",
    "    response, extracted_links = qa_chain.run(question)\n",
    "    return jsonify({\n",
    "        \"response\": response,\n",
    "        \"links\": extracted_links\n",
    "    })\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info(\"üöÄ Starting Flask application...\")\n",
    "    app.run(host='0.0.0.0', port=3000, debug=True)\n",
    "```\n",
    "\n",
    "Nh·ªØng thay ƒë·ªïi ch√≠nh:\n",
    "1. Di chuy·ªÉn kh·ªüi t·∫°o components ra ngo√†i route\n",
    "2. S·ª≠ d·ª•ng global variables ƒë·ªÉ l∆∞u tr·ªØ instances\n",
    "3. Ch·ªâ kh·ªüi t·∫°o m·ªôt l·∫ßn khi server start\n",
    "\n",
    "L·ª£i √≠ch:\n",
    "- Gi·∫£m th·ªùi gian load model\n",
    "- Tr√°nh kh·ªüi t·∫°o l·∫°i m·ªói khi c√≥ request\n",
    "- Ti·∫øt ki·ªám t√†i nguy√™n\n",
    "- V·∫´n gi·ªØ ƒë∆∞·ª£c debug mode n·∫øu c·∫ßn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi update th√¨ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "D:\\OneDrive - Hanoi University of Science and Technology\\GIT\\RAG-business-analysis\\src\\rag_pipeline\\back.py:56: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-qdrant package and should be used instead. To use it run `pip install -U :class:`~langchain-qdrant` and import as `from :class:`~langchain_qdrant import Qdrant``.\n",
    "  return Qdrant(\n",
    "2024-12-19 20:27:10,909 - INFO - üöÄ Starting Flask application...\n",
    "2024-12-19 20:27:10,917 - WARNING -  * Debugger is active!\n",
    "2024-12-19 20:27:10,934 - INFO -  * Debugger PIN: 100-469-070\n",
    "What is the name of this [person/place/thing/organization/etc.]?  Please provide more context if possible, such as a description or relevant details.\n",
    "\n",
    "2024-12-19 20:27:14,390 - INFO - HTTP Request: POST https://5d9673e8-d966-4738-adbb-95a5842604ba.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/cmc_final_db/points/search \"HTTP/1.1 200 OK\"\n",
    "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding. \n",
    "Error processing neighbors for doc 5852: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 9586: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 34622: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 39602: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 8587: Qdrant does not yet support get_by_ids.\n",
    "2024-12-19 20:27:34,562 - INFO - 127.0.0.1 - - [19/Dec/2024 20:27:34] \"POST /chat HTTP/1.1\" 200 -\n",
    "What is the name of this [person/place/thing/organization/file/etc.]?  Please provide more context if possible.\n",
    "\n",
    "2024-12-19 20:27:41,318 - INFO - HTTP Request: POST https://5d9673e8-d966-4738-adbb-95a5842604ba.europe-west3-0.gcp.cloud.qdrant.io:6333/collections/cmc_final_db/points/search \"HTTP/1.1 200 OK\"\n",
    "Error processing neighbors for doc 5852: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 34622: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 39602: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 1376: Qdrant does not yet support get_by_ids.\n",
    "Error processing neighbors for doc 37454: Qdrant does not yet support get_by_ids.\n",
    "2024-12-19 20:27:48,593 - INFO - 127.0.0.1 - - [19/Dec/2024 20:27:48] \"POST /chat HTTP/1.1\" 200 -\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
